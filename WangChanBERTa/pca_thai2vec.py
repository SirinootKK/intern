# -*- coding: utf-8 -*-
"""PCA thai2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vxxd4OZWDggGnkzUA984mM0T47O9dgxI

How to make Thai QA System using SimpleTransformer
- Pretrain Model: [Wangchanberta](https://medium.com/airesearch-in-th/wangchanberta-%E0%B9%82%E0%B8%A1%E0%B9%80%E0%B8%94%E0%B8%A5%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%A1%E0%B8%A7%E0%B8%A5%E0%B8%9C%E0%B8%A5%E0%B8%A0%E0%B8%B2%E0%B8%A9%E0%B8%B2%E0%B9%84%E0%B8%97%E0%B8%A2%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B9%83%E0%B8%AB%E0%B8%8D%E0%B9%88%E0%B9%81%E0%B8%A5%E0%B8%B0%E0%B8%81%E0%B9%89%E0%B8%B2%E0%B8%A7%E0%B8%AB%E0%B8%99%E0%B9%89%E0%B8%B2%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%AA%E0%B8%B8%E0%B8%94%E0%B9%83%E0%B8%99%E0%B8%82%E0%B8%93%E0%B8%B0%E0%B8%99%E0%B8%B5%E0%B9%89-d920c27cd433)
- Training: [Simple Transformer QA](https://simpletransformers.ai/docs/qa-minimal-start/)
- Author: Kobkrit Viriyayudhakorn [kobkrit@iapp.co.th](mailto:kobkrit@iapp.co.th)
- Written on 14 Apr 2022
-Modified by Kietikul Jearanaitanakij (Apr 23, 2023)
- Later Modified by Sirinoot Ketkham (Jun
 19, 2023)

https://simpletransformers.ai/docs/qa-model/

# **UPDATE**


---


- score : string Match
- ลด Dimension โดยใช้ TSNE 3~10
- ดูว่าควร train หยุดที่ epochs ไหนโดยดูจาก **ระนาบของ graph**

# Step 0 : Install dependencies
"""

! pip install -q simpletransformers

! pip install pythainlp

from sklearn.metrics.pairwise import cosine_similarity  # ใช้หาค่าความคล้ายคลึง
import numpy as np
import pandas as pd

#from datasets import load_dataset
#from transformers import AutoTokenizer
import numpy as np
import pandas as pd

dataset_path = "https://raw.githubusercontent.com/SirinootKK/intern/main/dataset/BERTa_ThaiQA_Corpus_1500.csv"

# from google.colab import files,drive
# drive.mount('/content/gdrive')
# dataset_path = '/content/gdrive/MyDrive/data/Full_ThaiQA_Corpus_wangchanberta.csv'

df = pd.read_csv(dataset_path)
df.dropna(inplace=True)
len(df)

"""# Step 1 : Preparing data"""

from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.2, random_state = 217)
train, valid = train_test_split(train, test_size=0.25, random_state = 217)
print(len(train) , ' ' , len(valid), ' ' , len(test))
train.head()

train_question_id = []
train_article_id = []
valid_question_id = []
valid_article_id = []
test_question_id = []
test_article_id = []

for i in range(len(train)):
  train_question_id.append(str(i))
  train_article_id.append(str(i))

for i in range(len(valid)):
  valid_question_id.append(str(i+len(train)))
  valid_article_id.append(str(i+len(train)))

for i in range(len(test)):
  test_question_id.append(str(i+len(train)+len(valid)))
  test_article_id.append(str(i+len(train)+len(valid)))

!pip install datasets

import datasets
from datasets import Dataset
train_set = { 'question_id': train_question_id,
              'article_id': train_article_id,
              'context' : train['context'],
              'question' : train['question'],
              'answer' : train['answer']
              #'answers' : Dataset.from_dict( { 'text': list(train['answer']), 'answer_start': list(train['answer_start'])} )
            }
valid_set = { 'question_id': valid_question_id,
              'article_id': valid_article_id,
              'context' : valid['context'],
              'question' : valid['question'],
              'answer' : valid['answer']
              #'answers' : Dataset.from_dict( { 'text': list(valid['answer']), 'answer_start': list(valid['answer_start'])} )
            }
test_set  = { 'question_id': test_question_id,
              'article_id': test_article_id,
              'context' : test['context'],
              'question' : test['question'],
              'answer' : test['answer']
              #'answers' : Dataset.from_dict( { 'text': list(test['answer']), 'answer_start': list(test['answer_start'])} )
            }
raw_datasets = datasets.DatasetDict({
    'train': Dataset.from_dict(train_set),
    'validation': Dataset.from_dict(valid_set),
    'test': Dataset.from_dict(test_set)})

raw_datasets['train']['context'][0]

import pandas as pd
train_df = pd.DataFrame(raw_datasets['train'])
validation_df = pd.DataFrame(raw_datasets['validation'])
test_df = pd.DataFrame(raw_datasets['test'])

"""# Step 2 : Word Tokenization

### สร้าง convert_to_simpletransformer_format
 method ไว้สำหรับแปลง format  Data เราให้ใช้กับ model Transformer ได้
"""

import json
from pythainlp.tokenize import word_tokenize

def convert_to_simpletransformer_format(df):
  all = {}
  for (idx, row) in df.iterrows():
    #print('*******')
    #print(row)
    #print('#######')
    answers = row["answer"]
    if (row["article_id"] not in all):
      all[row["article_id"]] = {"context": " ".join(word_tokenize(row['context'],engine="newmm-safe")), "qas":[]}
    question_text = " ".join(word_tokenize(row['question'],engine="newmm-safe"))
    #answer_text = " ".join(word_tokenize(answers["text"][0],engine="newmm-safe"))
    answer_text = " ".join(word_tokenize(answers,engine="newmm-safe"))
    try:
      answer_start = all[row["article_id"]]["context"].index(answer_text)
      all[row["article_id"]]["qas"].append({"id":row["question_id"], "is_impossible": False, "question": question_text, "answers": [{"text":answer_text, "answer_start": answer_start}]})
    except Exception as e:
      print("Context:" +all[row["article_id"]]["context"])
      print("Answer:" +answer_text)
      print("Exception:" +str(e))
  return list(all.values())

"""### ทดลองใช้ convert_to_simpletransformer_format"""

# test_df

"""#### Format data หลังทำการ convert to simpletransformer"""

convert_to_simpletransformer_format(test_df)

"""# Step 3 : Train the model

https://simpletransformers.ai/docs/qa-model/
"""

import logging
import torch

from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

train_data = convert_to_simpletransformer_format(train_df)
validation_data = convert_to_simpletransformer_format(validation_df)
test_data = convert_to_simpletransformer_format(test_df)

# Configure the model
model_args = QuestionAnsweringArgs()

model_args.loss_type = 'cross_entropy'

model_args.evaluate_during_training = True
model_args.save_best_model = True
model_args.overwrite_output_dir = True

model_args.train_batch_size = 32

model_args.num_train_epochs = 5

model_args.eval_batch_size = 16

model_args.dropout = 0.1
model_args.weight_decay = 0.01
model_args.learning_rate = 1e-05
model_args.adam_epsilon = 1e-06
# model_args.gradient_accumulation_steps = 8

model = QuestionAnsweringModel(
    "camembert", "airesearch/wangchanberta-base-att-spm-uncased", args=model_args, use_cuda=torch.cuda.is_available()
)

# !pip install wandb -qq
# import wandb
# import traceback

# Set the WANDB_TEAM environment variable
# WANDB_API_KEY = '5a6baf1210ed659768873e6c4f21ff540162cb77'
# wandb.login(key=WANDB_API_KEY)
# wandb.init(
#     entity="chatbot-chanan",
#     resume=True,
#     id = '8llncxv1',
#     project="WangchanBERTa",
#     name="id9_t3_e10_b32_ori"
#     )

# try:
model.train_model(train_data, eval_data=validation_data)
result, texts = model.eval_model(test_data)
#   wandb.save('/content/outputs/best_model/training_args.bin')
#   wandb.save('/content/outputs/best_model/pytorch_model.bin')
#   wandb.save('/content/outputs/training_progress_scores.csv')

# except Exception as e:
#   traceback.print_exc()
#   wandb.save("/content/outputs/training_args.bin")
#   wandb.save('/content/outputs/pytorch_model.bin')
#   try:
#     wandb.save('/content/outputs/training_progress_scores.csv')
#   except Exception as e:
#     print('error to save the scores file!!')
#     wandb.finish()

# Make predictions with the model
to_predict = [
    {
        "context": "ฟักแฟง ชาร์จ แบต อยู่",
        "qas": [
            {
                "question": "ฟักแฟง ทำ อะไร",
                "id": "0",
            }
        ],
    }
]

answers, probabilities = model.predict(to_predict)

print(answers)
print(probabilities)

#@title Graph
import pandas as pd
import matplotlib.pyplot as plt

# Read the CSV file
train_valid_loss = pd.read_csv('/content/outputs/training_progress_scores.csv')

# Extract the train_loss and eval_loss columns
train_loss = train_valid_loss['train_loss']
eval_loss = train_valid_loss['eval_loss']



# Create a list of epochs (assuming it is a sequential index)
epochs = train_valid_loss.index + 1

# Plot the training loss and evaluation loss
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, eval_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

"""# Build Chatbot by finding the most similar document using Thai2Vec"""

# from pythainlp import word_tokenize, word_vector  # ทำการเรียกตัวตัดคำ
# #from pythainlp.word_vector import * # ทำการเรียก thai2vec
# from sklearn.metrics.pairwise import cosine_similarity  # ใช้หาค่าความคล้ายคลึง
# import numpy as np

# wv_model = word_vector.WordVector(model_name="thai2fit_wv").get_model() # load thai2fit_wv from pythainlp

# #tsne 3-10 dimension

# #create dataframe
# thai2dict = {}
# for word in wv_model.index_to_key:
#     thai2dict[word] = wv_model[word]
# thai2vec = pd.DataFrame.from_dict(thai2dict,orient='index')
# thai2vec.head(10)

from sklearn.decomposition import PCA
from pythainlp import word_tokenize, word_vector
import numpy as np
from gensim.models import KeyedVectors

wv_model = word_vector.WordVector(model_name="thai2fit_wv").get_model()

class WordVectorReduced:
    def __init__(self, word_vectors_reduced, index_to_key):
        self.word_vectors_reduced = word_vectors_reduced
        self.index_to_key = index_to_key

    def get_vector(self, word):
        return self.word_vectors_reduced[self.index_to_key.index(word)]

word_vectors = [wv_model[word] for word in wv_model.index_to_key]
pca = PCA(n_components=10)
word_vectors_reduced = pca.fit_transform(word_vectors)

# Save WordVectorReduced to file
word_vectors_reduced_file = "word_vectors_reduced.npy"
np.save(word_vectors_reduced_file, word_vectors_reduced)

# Load the reduced word vectors as gensim KeyedVectors
word_vectors_reduced = np.load(word_vectors_reduced_file)
index_to_key_reduced = wv_model.index_to_key
wv_model_reduced = KeyedVectors(vector_size=10)
wv_model_reduced.add_vectors(keys=index_to_key_reduced, weights=word_vectors_reduced)

# wv_model_reduced,"----",wv_model

# thai2dict = {}
# for word in wv_model_reduced.index_to_key:
#     thai2dict[word] = wv_model_reduced[word]
# thai2vec = pd.DataFrame.from_dict(thai2dict,orient='index')
# thai2vec.head(10)

# ใช้ wv_model_reduced ในฟังก์ชัน sentence_vectorizer
def sentence_vectorizer_reduced(ss, dim=10, use_mean=True):
    s = word_tokenize(ss)
    vec = np.zeros((1,dim))
    for word in s:
        if word in wv_model_reduced.index_to_key:
            vec+= wv_model_reduced.get_vector(word)
        else: pass
    if use_mean: vec /= len(s)
    return vec

def sentence_similarity(s1,s2):
    return cosine_similarity(sentence_vectorizer_reduced(str(s1)),sentence_vectorizer_reduced(str(s2)))
    #return np.dot( np.array(sentence_vectorizer(str(s1))) , np.array(sentence_vectorizer(str(s2))) )

sentence_similarity("สวัสดีครับ", "สวัสดีคร้าบ")

# from google.colab import drive
# import os

# drive.mount('/content/drive')

# file_name = "/content/drive/MyDrive/berta/word_vectors_reduced.npy"
# folder_name = "/content/drive/MyDrive/berta/outputs.zip"

# with open(file_name, 'w') as file:
#     file.write('Hello, World!')
# with open(folder_name, "w") as file:
#         file.write("Content to save")

"""# Step 4 : Make predictions with the model

"""

# Make predictions with the model

question = "พิกซาร์สตูดิโอตั้งอยู่ที่เมืองอะไร"
#tokenized_question = word_tokenize(question)
MostSimilarContext = ""
max_sim = 0

for c in df['context']:
  #tokenized_c = word_tokenize(c)
  #print(c)
  sim = sentence_similarity(question, c)
  if max_sim < sim:
    max_sim = sim
    MostSimilarContext = c

print(MostSimilarContext)
print(max_sim)

to_predict = [
    {
        #"context" : context,
        "context" : MostSimilarContext,
        "qas": [
            {
                "question": question,
                "id": "0",
            }
        ],
    }
]

answers, probabilities = model.predict(to_predict)

print(answers)
print(probabilities)

"""# Step 5 : Evaluate the model"""

# def predict(model, question):
#     similarities = [sentence_similarity(question, c) for c in df['context']]
#     max_sim_index = np.argmax(similarities)
#     most_similar_context = df['context'][max_sim_index]

#     to_predict = [
#         {
#             "context": most_similar_context,
#             "qas": [
#                 {
#                     "question": question,
#                     "id": "0",
#                 }
#             ],
#         }
#     ]

#     answers, probabilities = model.predict(to_predict)
#     return answers[0]["answer"][0]

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from pythainlp.tokenize import word_tokenize

# Train Doc2Vec model
documents = [TaggedDocument(words=word_tokenize(c), tags=[str(i)]) for i, c in enumerate(test_df['context'])]
d2v_model = Doc2Vec(documents, vector_size=100, window=5, min_count=1, epochs=10)

def predict(model, doc2vec_model, question):
    question = question
    most_similar_context = ""
    max_sim = 0

    for c in test_df['context']:
        sim = sentence_similarity(doc2vec_model, question, c)
        if max_sim < sim:
            max_sim = sim
            most_similar_context = c

    to_predict = [
        {
            "context": most_similar_context,
            "qas": [
                {
                    "question": question,
                    "id": "0",
                }
            ],
        }
    ]

    answers, probabilities = model.predict(to_predict)
    return answers[0]["answer"][0]

# Example usage
question = "พิกซาร์สตูดิโอตั้งอยู่ที่เมืองอะไร"
prediction = predict(model, d2v_model, question)
print(prediction)

#@title predict all test data and save y_list to wandb
import os
import time

start_time = time.time()
current_dir = os.getcwd()

file_name = 'y_list4.txt'
file_path = os.path.join(current_dir, file_name)

y_pred_ls = []
y_true_ls = []
accls=[]
precls = []
recls=[]
f1ls=[]

for i in range(len(test_data)):
    y_pred = predict(model, d2v_model, test_data[i]["qas"][0]['question'])
    y_true = test_df.iloc[i][4]
    y_pred_ls.append(y_pred)
    y_true_ls.append(y_true)

    with open(file_path, 'a') as file:
        file.write(f'Loop at {i}:\n')
        file.write(f'y_pred: {y_pred}\n')
        file.write(f'y_true: {y_true}\n')
        file.write('=====================\n')

end_time = time.time()
elapsed_time = end_time - start_time
print("Execution time: {:.2f} seconds".format(elapsed_time))

# def predict(model,question):
#   question = question
#   #tokenized_question = word_tokenize(question)
#   MostSimilarContext = ""
#   max_sim = 0

#   for c in df['context']:
#     #tokenized_c = word_tokenize(c)
#     #print(c)
#     sim = sentence_similarity(question, c)
#     if max_sim < sim:
#       max_sim = sim
#       MostSimilarContext = c

#   # print(MostSimilarContext)
#   # print(max_sim)

#   to_predict = [
#       {
#           #"context" : context,
#           "context" : MostSimilarContext,
#           "qas": [
#               {
#                   "question": question,
#                   "id": "0",
#               }
#           ],
#       }
#   ]

#   answers, probabilities = model.predict(to_predict)

#   # print(answers)
#   # print(probabilities)
# #   return answers[0]["answer"][0]

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_fscore_support as score

#@title predict all test data and save y_list to wandb
import os
import time

start_time = time.time()
current_dir = os.getcwd()

file_name = 'y_list.txt'
file_path = os.path.join(current_dir, file_name)

y_pred_ls = []
y_true_ls = []
accls=[]
precls = []
recls=[]
f1ls=[]

for i in range(len(test_data)):
    # print('=====================',i,'===================')
  y_pred = predict(model,test_data[i]["qas"][0]['question'])
  # y_true = test_data[i]["qas"][0]['answers'][0]['text']
  y_true = test_df.iloc[i][4]
  y_pred_ls.append(y_pred)
  y_true_ls.append(y_true)

  with open(file_path, 'a') as file:
    file.write(f'Loop at {i}:\n')
    file.write(f'y_pred: {y_pred}\n')
    file.write(f'y_true: {y_true}\n')
    file.write('=====================\n')

end_time = time.time()
elapsed_time = end_time - start_time
print("Execution time: {:.2f} seconds".format(elapsed_time))

y_pred_ls,y_true_ls

len(test_df)

len(y_true_ls),len(y_pred_ls)

result_df = pd.DataFrame()

result_df['y_true'] = y_true_ls
result_df['y_pred'] = y_pred_ls

# result_df

"""#Step 6 : Measure Performance
- Bleu
- Bluert
- Rouge : Rouge-L Rouge-1 Rouge-2
- accuracy
- precision
- recalls
- f1
"""

# Commented out IPython magic to ensure Python compatibility.
#@title installation
!pip install rouge --q
!git clone https://github.com/google-research/bleurt.git
# %cd bleurt
!pip install .

from nltk.translate.bleu_score import sentence_bleu
from statistics import mean
import pythainlp

reference_sentences = y_true_ls
candidate_sentences = y_pred_ls

reference_tokens = [pythainlp.word_tokenize(sentence, engine='newmm-safe') for sentence in reference_sentences]
candidate_tokens = [pythainlp.word_tokenize(sentence, engine='newmm-safe') for sentence in candidate_sentences]


bleu_scores = []
for reference, candidate in zip(reference_tokens, candidate_tokens):
    score = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0))
    bleu_scores.append(score)

# for i, score in enumerate(bleu_scores):
#     print(f"BLEU score row# {i+1}: {score}")

print('BLEU score -> {}'.format(mean(bleu_scores)))

def tokenize_text_for_blurt (text):
  return " ".join(pythainlp.word_tokenize(text,engine="newmm-safe"))

ref_cand_token = pd.DataFrame()

ref_cand_token['y_true_tk'] = y_true_ls
ref_cand_token['y_pred_tk'] = y_pred_ls

ref_cand_token['y_true_tk'] = ref_cand_token['y_true_tk'].apply(tokenize_text_for_blurt)
ref_cand_token['y_pred_tk'] = ref_cand_token['y_pred_tk'].apply(tokenize_text_for_blurt)
# ref_cand_token = tokenize_text_for_blurt(ref_cand_token)

ref_cand_token

from bleurt import score

checkpoint = "bleurt/test_checkpoint"
references = ref_cand_token['y_true_tk']
candidates = ref_cand_token['y_pred_tk']

scorer = score.BleurtScorer(checkpoint)
bleurt_scores = scorer.score(references=references, candidates=candidates)
assert isinstance(bleurt_scores, list) and len(bleurt_scores) == len(bleurt_scores)
print('BLEURT Score -> {}'.format(mean(bleurt_scores)))

from rouge import Rouge
rouge = Rouge()

num = 0

# Calculate Rouge score for each sentence pair
for ref, cand, daf, pred_ls in zip(ref_cand_token['y_true_tk'], ref_cand_token['y_pred_tk'],test_df['context'],y_pred_ls):
    scores = rouge.get_scores(ref, cand)
    cand_con = rouge.get_scores(daf, pred_ls)
    # print(f"Rouge-L score: {scores}")
    rouge_l_scores = scores[0]['rouge-l']['f']
    rouge_1_scores = scores[0]['rouge-1']['f']
    rouge_2_scores = scores[0]['rouge-2']['f']

    # avg_rouge_l_score = sum(rouge_l_scores) / len(rouge_l_scores)
    # avg_rouge_1_score = sum(rouge_1_scores) / len(rouge_1_scores)
    # avg_rouge_2_score = sum(rouge_2_scores) / len(rouge_2_scores)

    con_scores= cand_con[0]['rouge-l']['f']

    # print(rouge_l_scores, num,cand)
    # print('context vs cand => {} i => {}' .format(con_scores, num))
    # print("=================================")

    if(rouge_l_scores > 0.5 and con_scores < 0.9):
      num += 1
      # acc_ls.append(rouge_l_scores)
      # print('index {}, rougeL {}' .format(num, rouge_l_scores))


acc = num/len(y_true_ls)

print('accuracy -> {}'.format(acc))
print('precision -> {}'.format(metrics.precision_score(y_true_ls,y_pred_ls,average='macro')))
print('recall -> {}'.format(metrics.recall_score(y_true_ls,y_pred_ls,average='macro')))
print('f1 score -> {}'.format(metrics.f1_score(y_true_ls,y_pred_ls,average='macro')))
print('ROUGE-1 -> {}' .format(rouge_1_scores))
print('ROUGE-2 -> {}' .format(rouge_2_scores))
print('ROUGE-L -> {}' .format(rouge_l_scores))
print('BLEURT Score -> {}'.format(mean(bleurt_scores)))
print('BLEU score -> {}'.format(mean(bleu_scores)))